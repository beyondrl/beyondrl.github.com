---
layout: default
title:  "Curiosity-driven Exploration by Self-supervised Prediction"
date:   2020-03-05
cover: "/assets/images/s5.png"
categories: jekyll update
summary: "The extrinsic rewards to the agent are extremely sparse. The authors of this paper turn to curiosity as instrisic reward signal and introduce the Intrinsic Curiosity Module (ICM)."
slides: https://docs.google.com/presentation/d/e/2PACX-1vRRci5orLUHt4Fd1SDg0kLqXrkRMHOmUzLifVTyDgdFg64cGmFfx5SeHkp-x5O4vN1w_MYtyJWH8yxg
paper: https://arxiv.org/abs/1705.05363
code: https://github.com/pathak22/noreward-rl
---

<div class="container mb-0.5 block shadowed">
  <h1 class="mt-1.5">{{page.title}}</h1>
<iframe src="{{page.slides}}/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1440" height="839" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<br>
  <h2 class="mt-1.5">Abstract</h2>

        In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: <i>VizDoom</i> and <i>Super Mario Bros</i>. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.
<br>
<ul style="overflow: hidden; display: block;">
    {% if page.code %}
    <li style="float:left"><a href="{{page.code}}">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="30px">
            <path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9 1.4.3 2.6.4 3.8.4 8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1-8.4 1.9-15.9 2.7-22.6 2.7-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1 10.5 0 20-3.4 25.6-6 2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8 0 0 1.6-.5 5-.5 8.1 0 26.4 3.1 56.6 24.1 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 30.2-21 48.5-24.1 56.6-24.1 3.4 0 5 .5 5 .5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5 1.2 0 2.6-.1 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path>
        </svg>Source code</a></li>
    {% endif %}
    {% if page.paper %}
    <li style="float:left"><a href="{{page.paper}}">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="30px">
            <path d="M288 48H136c-22.092 0-40 17.908-40 40v336c0 22.092 17.908 40 40 40h240c22.092 0 40-17.908 40-40V176L288 48zm-16 144V80l112 112H272z"></path>
        </svg>Read the paper</a></li>
    {% endif %}
</ul>
<br>
  <h2 class="mt-1.5">Authors</h2>
		<a href="https://people.eecs.berkeley.edu/~pathak/" target="_blank">Deepak Pathak</a>,
		<a href="https://people.eecs.berkeley.edu/~pulkitag/" target="_blank">Pulkit Agrawal</a>,
		<a href="https://people.eecs.berkeley.edu/~efros/" target="_blank">Alexei A. Efros</a>,
		<a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a><br>
	University of California, Berkeley

  	<h2 class="mt-1.5">Video</h2>

        <iframe width="800" height="450" src="https://www.youtube.com/embed/J3FHOyhUn3A" frameborder="0" allowfullscreen=""></iframe>
        <a href="https://pathak22.github.io/noreward-rl/">Also checkout their website & source code!</a>


</div>
