---
---
References
==========

@article{peng_mcp_2019,
  title = {{{MCP}}: {{Learning Composable Hierarchical Control}} with {{Multiplicative Compositional Policies}}},
  shorttitle = {{{MCP}}},
  author = {Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey},
  year = {2019},
  month = may,
  url = {http://arxiv.org/abs/1905.09808},
  urldate = {2020-12-18},
  abstract = {Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills quickly becomes prohibitive. Composable primitives that can be recombined to create a large variety of behaviors can be more suitable for modeling this combinatorial explosion. In this work, we propose multiplicative compositional policies (MCP), a method for learning reusable motor skills that can be composed to produce a range of complex behaviors. Our method factorizes an agent's skills into a collection of primitives, where multiple primitives can be activated simultaneously via multiplicative composition. This flexibility allows the primitives to be transferred and recombined to elicit new behaviors as necessary for novel tasks. We demonstrate that MCP is able to extract composable skills for highly complex simulated characters from pre-training tasks, such as motion imitation, and then reuse these skills to solve challenging continuous control tasks, such as dribbling a soccer ball to a goal, and picking up an object and transporting it to a target location.},
  archivePrefix = {arXiv},
  eprint = {1905.09808},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/W9M6REFS/Peng et al_2019_MCP.pdf;/home/tijs/Zotero/storage/DF3YNVJ7/1905.html},
  journal = {arXiv:1905.09808 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{stolle_learning_2002,
  title = {Learning {{Options}} in {{Reinforcement Learning}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Stolle, Martin and Precup, Doina},
  year = {2002},
  pages = {212--223},
  abstract = {Temporally extended actions (e.g., macro actions) have proven very  useful in speeding up learning, ensuring robustness and building prior knowledge  into AI systems. The options framework (Precup, 2000; Sutton, Precup \& Singh,  1999) provides a natural way of incorporating such actions into reinforcement  learning systems, but leaves open the issue of how good options might be identified.},
  file = {/home/tijs/Zotero/storage/NV48WNTI/Stolle and Precup - 2002 - Learning Options in Reinforcement Learning.pdf;/home/tijs/Zotero/storage/R8RGNYTS/summary.html}
}



