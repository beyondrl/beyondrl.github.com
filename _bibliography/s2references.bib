---
---
References
==========

@article{petosa_multiplayer_2019,
  title = {Multiplayer {{AlphaZero}}},
  author = {Petosa, Nick and Balch, Tucker},
  year = {2019},
  month = dec,
  url = {http://arxiv.org/abs/1910.13012},
  urldate = {2019-12-11},
  abstract = {The AlphaZero algorithm has achieved superhuman performance in two-player, deterministic, zero-sum games where perfect information of the game state is available. This success has been demonstrated in Chess, Shogi, and Go where learning occurs solely through self-play. Many real-world applications (e.g., equity trading) require the consideration of a multiplayer environment. In this work, we suggest novel modifications of the AlphaZero algorithm to support multiplayer environments, and evaluate the approach in two simple 3-player games. Our experiments show that multiplayer AlphaZero learns successfully and consistently outperforms a competing approach: Monte Carlo tree search. These results suggest that our modified AlphaZero can learn effective strategies in multiplayer game scenarios. Our work supports the use of AlphaZero in multiplayer games and suggests future research for more complex environments.},
  archivePrefix = {arXiv},
  eprint = {1910.13012},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/W3CYKQWM/Petosa and Balch - 2019 - Multiplayer AlphaZero.pdf;/home/tijs/Zotero/storage/66JBVJ22/1910.html},
  journal = {arXiv:1910.13012 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{schrittwieser_mastering_2019,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.08265},
  urldate = {2019-12-09},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archivePrefix = {arXiv},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/WAXPQS24/Schrittwieser et al. - 2019 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf;/home/tijs/Zotero/storage/SLFCLZG6/1911.html},
  journal = {arXiv:1911.08265 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2019-12-17},
  abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {/home/tijs/Zotero/storage/5FK25BTM/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf;/home/tijs/Zotero/storage/QSB4VPQZ/nature16961.html},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{silver_mastering_2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2019-12-11},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/FW7B3U62/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/home/tijs/Zotero/storage/YJCCSWSW/1712.html},
  journal = {arXiv:1712.01815 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}



