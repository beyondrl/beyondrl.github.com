---
---
References
==========

@article{santoro_relational_2018,
	title = {Relational recurrent neural networks},
	url = {http://arxiv.org/abs/1806.01822},
	abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a {\textbackslash}textit\{Relational Memory Core\} ({RMC}) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the {RMC} on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in {RL} domains (e.g. Mini {PacMan}), program evaluation, and language modeling, achieving state-of-the-art results on the {WikiText}-103, Project Gutenberg, and {GigaWord} datasets.},
	journaltitle = {{arXiv}:1806.01822 [cs, stat]},
	author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
	urldate = {2020-04-01},
	date = {2018-06-28},
	eprinttype = {arxiv},
	eprint = {1806.01822},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/tijs/Zotero/storage/DX5EATRP/Santoro et al. - 2018 - Relational recurrent neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/tijs/Zotero/storage/5JRPTH7L/1806.html:text/html}
}

@article{pritzel_neural_2017,
	title = {Neural Episodic Control},
	url = {http://arxiv.org/abs/1703.01988},
	abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
	journaltitle = {{arXiv}:1703.01988 [cs, stat]},
	author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdomènech, Adrià and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
	urldate = {2020-04-01},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1703.01988},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/tijs/Zotero/storage/4XK53JRP/Pritzel et al. - 2017 - Neural Episodic Control.pdf:application/pdf;arXiv.org Snapshot:/home/tijs/Zotero/storage/4KWPFRU2/1703.html:text/html}
}

@article{le_self-attentive_2020,
	title = {Self-Attentive Associative Memory},
	url = {http://arxiv.org/abs/2002.03519},
	abstract = {Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory ({SAM}) operator. Found upon outer product, {SAM} forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.},
	journaltitle = {{arXiv}:2002.03519 [cs, stat]},
	author = {Le, Hung and Tran, Truyen and Venkatesh, Svetha},
	urldate = {2020-04-01},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2002.03519},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/tijs/Zotero/storage/Q9QT675J/Le et al. - 2020 - Self-Attentive Associative Memory.pdf:application/pdf;arXiv.org Snapshot:/home/tijs/Zotero/storage/3K4VWFLQ/2002.html:text/html}
}

@article{graves_neural_2014,
	title = {Neural Turing Machines},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	journaltitle = {{arXiv}:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	urldate = {2020-06-02},
	date = {2014-12-10},
	eprinttype = {arxiv},
	eprint = {1410.5401},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/tijs/Zotero/storage/9SQNHVHE/1410.html:text/html;Graves et al_2014_Neural Turing Machines.pdf:/home/tijs/Zotero/storage/XHFA29TM/Graves et al_2014_Neural Turing Machines.pdf:application/pdf}
}

@article{weston_towards_2015,
	title = {Towards {AI}-Complete Question Answering: A Set of Prerequisite Toy Tasks},
	url = {http://arxiv.org/abs/1502.05698},
	shorttitle = {Towards {AI}-Complete Question Answering},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	journaltitle = {{arXiv}:1502.05698 [cs, stat]},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	urldate = {2020-06-02},
	date = {2015-12-31},
	eprinttype = {arxiv},
	eprint = {1502.05698},
	keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/tijs/Zotero/storage/48UKJ7VX/1502.html:text/html;Weston et al_2015_Towards AI-Complete Question Answering.pdf:/home/tijs/Zotero/storage/A68NQK76/Weston et al_2015_Towards AI-Complete Question Answering.pdf:application/pdf}
}