---
---
References
==========


@article{crawford_spatially_2019,
  title = {Spatially {{Invariant Unsupervised Object Detection}} with {{Convolutional Neural Networks}}},
  author = {Crawford, Eric and Pineau, Joelle},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {3412--3420},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33013412},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/4216},
  urldate = {2020-02-01},
  abstract = {There are many reasons to expect an ability to reason in terms of objects to be a crucial skill for any generally intelligent agent. Indeed, recent machine learning literature is replete with examples of the benefits of object-like representations: generalization, transfer to new tasks, and interpretability, among others. However, in order to reason in terms of objects, agents need a way of discovering and detecting objects in the visual world - a task which we call unsupervised object detection. This task has received significantly less attention in the literature than its supervised counterpart, especially in the case of large images containing many objects. In the current work, we develop a neural network architecture that effectively addresses this large-image, many-object setting. In particular, we combine ideas from Attend, Infer, Repeat (AIR), which performs unsupervised object detection but does not scale well, with recent developments in supervised object detection. We replace AIR's core recurrent network with a convolutional (and thus spatially invariant) network, and make use of an object-specification scheme that describes the location of objects with respect to local grid cells rather than the image as a whole. Through a series of experiments, we demonstrate a number of features of our architecture: that, unlike AIR, it is able to discover and detect objects in large, many-object scenes; that it has a significant ability to generalize to images that are larger and contain more objects than images encountered during training; and that it is able to discover and detect objects with enough accuracy to facilitate non-trivial downstream processing.},
  file = {/home/tijs/Zotero/storage/RCDCJ3PG/Crawford and Pineau - 2019 - Spatially Invariant Unsupervised Object Detection .pdf},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  language = {en}
}

@article{greff_multiobject_2019,
  title = {Multi-{{Object Representation Learning}} with {{Iterative Variational Inference}}},
  author = {Greff, Klaus and Kaufman, Rapha{\"e}l Lopez and Kabra, Rishabh and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  year = {2019},
  month = may,
  url = {http://arxiv.org/abs/1903.00450},
  urldate = {2020-01-22},
  abstract = {Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.},
  archivePrefix = {arXiv},
  eprint = {1903.00450},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/JDPNI9YK/Greff et al. - 2019 - Multi-Object Representation Learning with Iterativ.pdf;/home/tijs/Zotero/storage/E22JEF5F/1903.html},
  journal = {arXiv:1903.00450 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ha_world_2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  doi = {10.5281/zenodo.1207631},
  url = {http://arxiv.org/abs/1803.10122},
  urldate = {2020-02-01},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archivePrefix = {arXiv},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/ECS8QIX4/Ha and Schmidhuber - 2018 - World Models.pdf;/home/tijs/Zotero/storage/QAS7KKBL/1803.html},
  journal = {arXiv:1803.10122 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kipf_contrastive_2020,
  title = {Contrastive {{Learning}} of {{Structured World Models}}},
  author = {Kipf, Thomas and {van der Pol}, Elise and Welling, Max},
  year = {2020},
  month = jan,
  url = {http://arxiv.org/abs/1911.12247},
  urldate = {2020-01-22},
  abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
  archivePrefix = {arXiv},
  eprint = {1911.12247},
  eprinttype = {arxiv},
  file = {/home/tijs/Zotero/storage/C6DITXBX/Kipf et al. - 2020 - Contrastive Learning of Structured World Models.pdf;/home/tijs/Zotero/storage/P5ADGDHQ/1911.html},
  journal = {arXiv:1911.12247 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{nguyen_deepusps_2019,
  title = {{{DeepUSPS}}: {{Deep Robust Unsupervised Saliency Prediction}} via {{Self}}-Supervision},
  shorttitle = {{{DeepUSPS}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Nguyen, Tam and Dax, Maximilian and Mummadi, Chaithanya Kumar and Ngo, Nhung and Nguyen, Thi Hoai Phuong and Lou, Zhongyu and Brox, Thomas},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {204--214},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/8314-deepusps-deep-robust-unsupervised-saliency-prediction-via-self-supervision.pdf},
  urldate = {2020-02-11},
  file = {/home/tijs/Zotero/storage/KKKQVTM3/Nguyen et al. - 2019 - DeepUSPS Deep Robust Unsupervised Saliency Predic.pdf;/home/tijs/Zotero/storage/EZ9GV2Q7/8314-deepusps-deep-robust-unsupervised-saliency-prediction-via-self-supervision.html}
}



